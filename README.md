# ğŸ“š DocuMind AI

> Multimodal Retrieval-Augmented Generation (RAG) System for Intelligent Document Understanding

## ğŸ¯ Overview

DocuMind AI is an experimental multimodal RAG system designed to understand and answer questions about PDF and Word documents. This project explores cutting-edge techniques in semantic chunking, cross-modal embeddings, and context-aware language generation.

## ğŸ”¥ Key Features

- ğŸ“„ Multimodal Document Processing - Extract text and images from PDFs and Word documents
- ğŸ–¼ï¸ Cross-modal Search - Query across text and images in a unified embedding space
- ğŸš€ Production-Ready Deployment - End-to-end deployment on Hugging Face Spaces
- âš¡ Real-time Q&A - Get intelligent answers from your documents instantly
- ğŸ”¬ Experimental Framework - Designed for rapid iteration and technique exploration

## ğŸŒ Live Demo

Try DocuMind AI on Hugging Face Spaces:
https://huggingface.co/spaces/Sakshi0104/documind-ai

## ğŸ—ï¸ Architecture

PDF/Word Document
    â†“
Text + Image Extraction (PyMuPDF, python-docx)
    â†“
Semantic Chunking (LangChain - 500 chars, 50 overlap)
    â†“
Multimodal Embeddings (CLIP openai/clip-vit-base-patch32)
    â†“
Vector Store (FAISS - CPU Index)
    â†“
Semantic Search (Top-K=3 Retrieval)
    â†“
Context + Query â†’ LLM (Groq - Llama 3.1 8B)
    â†“
Intelligent Answer

## ğŸ§  Technical Stack

Current Implementation:
- Chunking: Fixed-Size Character (500 chars, 50 overlap)
- Embeddings: CLIP ViT-Base-32 (512-dim vectors)
- Vector DB: FAISS (CPU-optimized)
- LLM: Llama 3.1 8B (Groq API)
- Framework: LangChain (RAG orchestration)
- UI: Gradio 4.44.0 (Web interface)

## ğŸ“¦ Installation & Local Deployment

Prerequisites:
- Python 3.10+
- pip / conda
- Git

Setup Steps:

1. Clone Repository:
git clone https://github.com/Sakshiv0104/DocuMind-AI.git
cd DocuMind-AI

2. Create Virtual Environment:
python -m venv venv
source venv/bin/activate

3. Install Dependencies:
pip install -r requirements.txt

4. Set Environment Variables:
Create .env file with:
GROQ_API_KEY=your_groq_api_key_here

5. Run Locally:
python app.py
Access at http://localhost:7860

## ğŸš€ Hugging Face Spaces Deployment

Live Link: https://huggingface.co/spaces/Sakshi0104/documind-ai
Status: Running (Free Tier - CPU)
Cold Start: ~5-10 minutes (CLIP model loading)
Warm State: 30 seconds per query

## ğŸ”§ Configuration

Edit config.py to customize:

CHUNK_SIZE = 500              # Characters per chunk
CHUNK_OVERLAP = 50            # Overlap between chunks
TOP_K = 3                     # Number of chunks to retrieve
MAX_PAGES_PER_PDF = 5         # Pages to process per PDF
EMBEDDING_MODEL = "openai/clip-vit-base-patch32"
LLM_MODEL = "llama-3.1-8b-instant"
TEMPERATURE = 0.7
MAX_TOKENS = 1024

## ğŸ“Š Performance

Cold Start: 5-10 min
Warm Query: ~30 sec
Embedding Time: 2-5 sec
Retrieval Speed: <100ms

## ğŸ¯ Vision

DocuMind AI represents my exploration of modern AI techniques in document understanding and retrieval-augmented generation. Through systematic experimentation with different chunking strategies, embedding models, and vector databases, I aim to build a robust, adaptable system that demonstrates best practices in multimodal AI.

This project showcases end-to-end deployment from local development through production deployment on Hugging Face Spaces. As I continue this journey, I aspire to contribute meaningfully to the open-source AI community and explore emerging techniques in semantic search, prompt engineering, and knowledge retrieval systems.

## ğŸ”— Quick Links

Live App: https://huggingface.co/spaces/Sakshi0104/documind-ai
GitHub: https://github.com/Sakshiv0104/DocuMind-AI

Status: In Active Development - Expect updates and experimental features!
